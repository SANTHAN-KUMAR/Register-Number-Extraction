{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672a1d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4276, Val: 535, Test: 535\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set up the data directory\n",
    "data_dir = 'Register Numbers/'\n",
    "\n",
    "# Load image paths and labels\n",
    "image_pairs = [\n",
    "    (os.path.join(data_dir, f), f.split('.')[0]) \n",
    "    for f in os.listdir(data_dir) \n",
    "    if f.endswith('.png')\n",
    "]\n",
    "\n",
    "# Validate labels\n",
    "def validate_label(label):\n",
    "    if len(label) != 12 or not label.isdigit():\n",
    "        raise ValueError(f\"Invalid register number: {label}\")\n",
    "    return label\n",
    "\n",
    "cleaned_pairs = [(path, validate_label(label[:12])) for path, label in image_pairs]\n",
    "\n",
    "# Split the data\n",
    "random.shuffle(cleaned_pairs)\n",
    "total = len(cleaned_pairs)\n",
    "train = cleaned_pairs[:int(0.8 * total)]\n",
    "val = cleaned_pairs[int(0.8 * total):int(0.9 * total)]\n",
    "test = cleaned_pairs[int(0.9 * total):]\n",
    "print(f\"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b041f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegisterNumberDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('L')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label_seq = [int(digit) + 1 for digit in label]  # 0->1, 1->2, ..., 9->10\n",
    "        return image, torch.tensor(label_seq, dtype=torch.long)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = RegisterNumberDataset([p for p, _ in train], [l for _, l in train], transform=transform)\n",
    "val_dataset = RegisterNumberDataset([p for p, _ in val], [l for _, l in val], transform=transform)\n",
    "test_dataset = RegisterNumberDataset([p for p, _ in test], [l for _, l in test], transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6962406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegisterNumberDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('L')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label_seq = [int(digit) + 1 for digit in label]  # 0->1, 1->2, ..., 9->10\n",
    "        return image, torch.tensor(label_seq, dtype=torch.long)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = RegisterNumberDataset([p for p, _ in train], [l for _, l in train], transform=transform)\n",
    "val_dataset = RegisterNumberDataset([p for p, _ in val], [l for _, l in val], transform=transform)\n",
    "test_dataset = RegisterNumberDataset([p for p, _ in test], [l for _, l in test], transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd23d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Dropout2d(p=0.3, inplace=False)\n",
       "    (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): ReLU()\n",
       "    (22): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (23): Dropout2d(p=0.3, inplace=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(2, 1), stride=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU()\n",
       "  )\n",
       "  (rnn): LSTM(512, 256, num_layers=2, dropout=0.3, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CRNN, self).__init__()\n",
    "        # CNN component with dropout\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # (N, 64, 16, W/2)\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # (N, 128, 8, W/4)\n",
    "            nn.Dropout2d(0.3),  # Dropout after second maxpool\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,1), (2,1)),  # (N, 256, 4, W/4)\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,1), (2,1)),  # (N, 512, 2, W/4)\n",
    "            nn.Dropout2d(0.3),  # Dropout after fourth maxpool\n",
    "            nn.Conv2d(512, 512, kernel_size=(2,1)),  # (N, 512, 1, W/4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # LSTM with dropout between layers\n",
    "        self.rnn = nn.LSTM(512, 256, num_layers=2, bidirectional=True, dropout=0.3)\n",
    "        # Dropout before the fully connected layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512, num_classes)  # 512 because bidirectional (256 * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through CNN\n",
    "        x = self.cnn(x)  # (N, 512, 1, W/4)\n",
    "        x = x.squeeze(2)  # (N, 512, W/4)\n",
    "        x = x.permute(2, 0, 1)  # (W/4, N, 512) for LSTM\n",
    "        # Pass through LSTM\n",
    "        x, _ = self.rnn(x)  # (W/4, N, 512)\n",
    "        # Apply dropout before FC\n",
    "        x = self.dropout(x)\n",
    "        # Fully connected layer for classification\n",
    "        x = self.fc(x)  # (W/4, N, num_classes)\n",
    "        return x\n",
    "\n",
    "# Initialize the model (assuming 11 classes: blank + digits 0-9)\n",
    "model = CRNN(num_classes=11)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f9884ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70, Train Loss: 1.5128, Val Loss: 1.0863, Train Acc: 0.00%, Val Acc: 0.00%\n",
      "Epoch 2/70, Train Loss: 0.8534, Val Loss: 0.6677, Train Acc: 0.00%, Val Acc: 0.00%\n",
      "Epoch 3/70, Train Loss: 0.5524, Val Loss: 0.4512, Train Acc: 1.26%, Val Acc: 2.43%\n",
      "Epoch 4/70, Train Loss: 0.3689, Val Loss: 0.2421, Train Acc: 9.71%, Val Acc: 24.49%\n",
      "Epoch 5/70, Train Loss: 0.2285, Val Loss: 0.1672, Train Acc: 37.35%, Val Acc: 57.01%\n",
      "Epoch 6/70, Train Loss: 0.1439, Val Loss: 0.1004, Train Acc: 63.99%, Val Acc: 75.70%\n",
      "Epoch 7/70, Train Loss: 0.1116, Val Loss: 0.0926, Train Acc: 75.00%, Val Acc: 82.80%\n",
      "Epoch 8/70, Train Loss: 0.0778, Val Loss: 0.0658, Train Acc: 84.07%, Val Acc: 87.48%\n",
      "Epoch 9/70, Train Loss: 0.0677, Val Loss: 0.0854, Train Acc: 85.97%, Val Acc: 81.50%\n",
      "Epoch 10/70, Train Loss: 0.0842, Val Loss: 0.1217, Train Acc: 82.97%, Val Acc: 76.26%\n",
      "Epoch 11/70, Train Loss: 0.0728, Val Loss: 0.0637, Train Acc: 85.17%, Val Acc: 88.97%\n",
      "Epoch 12/70, Train Loss: 0.0539, Val Loss: 0.0650, Train Acc: 89.36%, Val Acc: 88.41%\n",
      "Epoch 13/70, Train Loss: 0.0459, Val Loss: 0.0562, Train Acc: 91.35%, Val Acc: 90.09%\n",
      "Epoch 14/70, Train Loss: 0.0436, Val Loss: 0.0551, Train Acc: 91.18%, Val Acc: 90.28%\n",
      "Epoch 15/70, Train Loss: 0.0433, Val Loss: 0.0623, Train Acc: 91.77%, Val Acc: 88.04%\n",
      "Epoch 16/70, Train Loss: 0.0386, Val Loss: 0.0592, Train Acc: 92.59%, Val Acc: 89.91%\n",
      "Epoch 17/70, Train Loss: 0.0370, Val Loss: 0.0505, Train Acc: 93.12%, Val Acc: 92.15%\n",
      "Epoch 18/70, Train Loss: 0.0302, Val Loss: 0.0463, Train Acc: 94.32%, Val Acc: 90.65%\n",
      "Epoch 19/70, Train Loss: 0.0282, Val Loss: 0.0549, Train Acc: 94.67%, Val Acc: 91.03%\n",
      "Epoch 20/70, Train Loss: 0.0273, Val Loss: 0.0542, Train Acc: 94.97%, Val Acc: 91.03%\n",
      "Epoch 21/70, Train Loss: 0.0252, Val Loss: 0.0550, Train Acc: 95.16%, Val Acc: 91.21%\n",
      "Epoch 22/70, Train Loss: 0.0304, Val Loss: 0.0544, Train Acc: 94.08%, Val Acc: 90.28%\n",
      "Epoch 23/70, Train Loss: 0.0242, Val Loss: 0.0556, Train Acc: 95.23%, Val Acc: 91.03%\n",
      "Epoch 24/70, Train Loss: 0.0254, Val Loss: 0.0514, Train Acc: 95.32%, Val Acc: 91.21%\n",
      "Epoch 25/70, Train Loss: 0.0661, Val Loss: 0.1060, Train Acc: 90.27%, Val Acc: 73.64%\n",
      "Epoch 26/70, Train Loss: 0.0485, Val Loss: 0.0484, Train Acc: 89.24%, Val Acc: 90.84%\n",
      "Epoch 27/70, Train Loss: 0.0314, Val Loss: 0.0486, Train Acc: 93.73%, Val Acc: 90.84%\n",
      "Epoch 28/70, Train Loss: 0.0275, Val Loss: 0.0563, Train Acc: 94.50%, Val Acc: 91.03%\n",
      "Epoch 29/70, Train Loss: 0.0290, Val Loss: 0.0607, Train Acc: 94.53%, Val Acc: 90.47%\n",
      "Epoch 30/70, Train Loss: 0.0239, Val Loss: 0.0539, Train Acc: 94.93%, Val Acc: 90.84%\n",
      "Epoch 31/70, Train Loss: 0.0268, Val Loss: 0.0544, Train Acc: 94.64%, Val Acc: 92.90%\n",
      "Epoch 32/70, Train Loss: 0.0205, Val Loss: 0.0540, Train Acc: 95.79%, Val Acc: 92.15%\n",
      "Epoch 33/70, Train Loss: 0.0182, Val Loss: 0.0526, Train Acc: 96.33%, Val Acc: 92.34%\n",
      "Epoch 34/70, Train Loss: 0.0185, Val Loss: 0.0546, Train Acc: 96.09%, Val Acc: 91.96%\n",
      "Epoch 35/70, Train Loss: 0.0162, Val Loss: 0.0540, Train Acc: 96.42%, Val Acc: 91.96%\n",
      "Epoch 36/70, Train Loss: 0.0145, Val Loss: 0.0569, Train Acc: 97.05%, Val Acc: 92.52%\n",
      "Epoch 37/70, Train Loss: 0.0177, Val Loss: 0.0616, Train Acc: 96.07%, Val Acc: 90.84%\n",
      "Epoch 38/70, Train Loss: 0.0187, Val Loss: 0.0538, Train Acc: 95.93%, Val Acc: 92.71%\n",
      "Epoch 39/70, Train Loss: 0.0174, Val Loss: 0.0573, Train Acc: 95.98%, Val Acc: 91.96%\n",
      "Epoch 40/70, Train Loss: 0.0168, Val Loss: 0.0573, Train Acc: 96.42%, Val Acc: 91.78%\n",
      "Epoch 41/70, Train Loss: 0.0163, Val Loss: 0.0534, Train Acc: 96.61%, Val Acc: 91.96%\n",
      "Epoch 42/70, Train Loss: 0.0114, Val Loss: 0.0576, Train Acc: 97.66%, Val Acc: 91.78%\n",
      "Epoch 43/70, Train Loss: 0.0138, Val Loss: 0.0580, Train Acc: 96.89%, Val Acc: 91.96%\n",
      "Epoch 44/70, Train Loss: 0.0139, Val Loss: 0.0556, Train Acc: 96.80%, Val Acc: 92.15%\n",
      "Epoch 45/70, Train Loss: 0.0169, Val Loss: 0.0693, Train Acc: 96.23%, Val Acc: 90.65%\n",
      "Epoch 46/70, Train Loss: 0.0176, Val Loss: 0.0561, Train Acc: 95.98%, Val Acc: 91.78%\n",
      "Epoch 47/70, Train Loss: 0.0143, Val Loss: 0.0428, Train Acc: 96.82%, Val Acc: 94.02%\n",
      "Epoch 48/70, Train Loss: 0.0157, Val Loss: 0.0570, Train Acc: 96.54%, Val Acc: 91.96%\n",
      "Epoch 49/70, Train Loss: 0.0179, Val Loss: 0.0592, Train Acc: 95.51%, Val Acc: 91.59%\n",
      "Epoch 50/70, Train Loss: 0.0116, Val Loss: 0.0621, Train Acc: 97.29%, Val Acc: 91.40%\n",
      "Epoch 51/70, Train Loss: 0.0124, Val Loss: 0.0598, Train Acc: 97.03%, Val Acc: 91.59%\n",
      "Epoch 52/70, Train Loss: 0.0118, Val Loss: 0.0516, Train Acc: 96.98%, Val Acc: 92.90%\n",
      "Epoch 53/70, Train Loss: 0.0115, Val Loss: 0.0574, Train Acc: 97.03%, Val Acc: 92.15%\n",
      "Epoch 54/70, Train Loss: 0.0125, Val Loss: 0.0597, Train Acc: 96.70%, Val Acc: 91.21%\n",
      "Epoch 55/70, Train Loss: 0.0159, Val Loss: 0.0641, Train Acc: 95.86%, Val Acc: 91.40%\n",
      "Epoch 56/70, Train Loss: 0.0127, Val Loss: 0.0692, Train Acc: 96.80%, Val Acc: 90.84%\n",
      "Epoch 57/70, Train Loss: 0.0097, Val Loss: 0.0896, Train Acc: 97.36%, Val Acc: 88.22%\n",
      "Epoch 58/70, Train Loss: 0.0220, Val Loss: 0.0608, Train Acc: 94.29%, Val Acc: 90.84%\n",
      "Epoch 59/70, Train Loss: 0.0181, Val Loss: 0.0648, Train Acc: 95.70%, Val Acc: 89.53%\n",
      "Epoch 60/70, Train Loss: 0.0130, Val Loss: 0.0606, Train Acc: 96.52%, Val Acc: 91.59%\n",
      "Epoch 61/70, Train Loss: 0.0102, Val Loss: 0.0617, Train Acc: 97.61%, Val Acc: 92.71%\n",
      "Epoch 62/70, Train Loss: 0.0159, Val Loss: 0.0683, Train Acc: 96.28%, Val Acc: 91.40%\n",
      "Epoch 63/70, Train Loss: 0.0108, Val Loss: 0.0702, Train Acc: 96.89%, Val Acc: 91.40%\n",
      "Epoch 64/70, Train Loss: 0.0078, Val Loss: 0.0682, Train Acc: 97.80%, Val Acc: 90.84%\n",
      "Epoch 65/70, Train Loss: 0.0087, Val Loss: 0.0643, Train Acc: 97.68%, Val Acc: 91.96%\n",
      "Epoch 66/70, Train Loss: 0.0083, Val Loss: 0.0777, Train Acc: 97.90%, Val Acc: 90.65%\n",
      "Epoch 67/70, Train Loss: 0.0125, Val Loss: 0.0644, Train Acc: 96.94%, Val Acc: 91.96%\n",
      "Epoch 68/70, Train Loss: 0.0089, Val Loss: 0.0736, Train Acc: 97.57%, Val Acc: 90.09%\n",
      "Epoch 69/70, Train Loss: 0.0079, Val Loss: 0.0741, Train Acc: 97.92%, Val Acc: 91.40%\n",
      "Epoch 70/70, Train Loss: 0.0061, Val Loss: 0.0633, Train Acc: 98.32%, Val Acc: 93.27%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CTCLoss(blank=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Decoding function (used in both training and evaluation)\n",
    "def decode_predictions(outputs):\n",
    "    outputs = outputs.softmax(2).argmax(2)  # (T, N)\n",
    "    batch_size = outputs.size(1)\n",
    "    decoded = []\n",
    "    for b in range(batch_size):\n",
    "        seq = outputs[:, b].cpu().numpy()\n",
    "        prev = -1\n",
    "        result = []\n",
    "        for s in seq:\n",
    "            if s != 0 and s != prev:\n",
    "                result.append(s - 1)\n",
    "            prev = s\n",
    "        decoded.append(''.join(map(str, result)))\n",
    "    return decoded\n",
    "\n",
    "# Training loop with validation\n",
    "num_epochs = 70\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # (W/4, N, num_classes)\n",
    "        outputs_log = outputs.log_softmax(2)\n",
    "        batch_size = images.size(0)\n",
    "        input_lengths = torch.full((batch_size,), outputs.size(0), dtype=torch.long, device=device)\n",
    "        target_lengths = torch.full((batch_size,), 12, dtype=torch.long, device=device)\n",
    "        targets_flat = targets.view(-1)\n",
    "        loss = criterion(outputs_log, targets_flat, input_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Compute training accuracy\n",
    "        predictions = decode_predictions(outputs)\n",
    "        target_labels = [''.join(str(d - 1) for d in t.tolist()) for t in targets]\n",
    "        for pred, target in zip(predictions, target_labels):\n",
    "            if pred == target:\n",
    "                train_correct += 1\n",
    "            train_total += 1\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = train_correct / train_total * 100\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs_log = outputs.log_softmax(2)\n",
    "            batch_size = images.size(0)\n",
    "            input_lengths = torch.full((batch_size,), outputs.size(0), dtype=torch.long, device=device)\n",
    "            target_lengths = torch.full((batch_size,), 12, dtype=torch.long, device=device)\n",
    "            targets_flat = targets.view(-1)\n",
    "            loss = criterion(outputs_log, targets_flat, input_lengths, target_lengths)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            predictions = decode_predictions(outputs)\n",
    "            target_labels = [''.join(str(d - 1) for d in t.tolist()) for t in targets]\n",
    "            for pred, target in zip(predictions, target_labels):\n",
    "                if pred == target:\n",
    "                    val_correct += 1\n",
    "                val_total += 1\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = val_correct / val_total * 100\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a5431e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.03%\n"
     ]
    }
   ],
   "source": [
    "def decode_predictions(outputs):\n",
    "    outputs = outputs.softmax(2).argmax(2)  # (T, N) -> e.g., (64, N)\n",
    "    batch_size = outputs.size(1)\n",
    "    decoded = []\n",
    "    for b in range(batch_size):\n",
    "        seq = outputs[:, b].cpu().numpy()  # (T,)\n",
    "        prev = -1\n",
    "        result = []\n",
    "        for s in seq:\n",
    "            if s != 0 and s != prev:\n",
    "                result.append(s - 1)\n",
    "            prev = s\n",
    "        decoded.append(''.join(map(str, result)))\n",
    "    return decoded\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)  # (T, N, num_classes)\n",
    "        predictions = decode_predictions(outputs)\n",
    "        target_labels = [''.join(str(d - 1) for d in t.tolist()) for t in targets]\n",
    "        for pred, target in zip(predictions, target_labels):\n",
    "            if pred == target:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(f\"Test Accuracy: {correct/total*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e943a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Register Number: 2122240066\n"
     ]
    }
   ],
   "source": [
    "def predict_register_number(model, image_path, device, transform):\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # (1, 1, 32, 256)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        output = model(image)  # (T, 1, num_classes), e.g., (64, 1, 11)\n",
    "        output = output.squeeze(1)  # (T, num_classes)\n",
    "        output = output.softmax(1).argmax(1)  # (T,)\n",
    "        seq = output.cpu().numpy()\n",
    "        prev = -1\n",
    "        result = []\n",
    "        for s in seq:\n",
    "            if s != 0 and s != prev:\n",
    "                result.append(s - 1)\n",
    "            prev = s\n",
    "    return ''.join(map(str, result))\n",
    "\n",
    "# Test with your image\n",
    "image_path = 'my_reg.png'  # Replace with your image path\n",
    "predicted_number = predict_register_number(model, image_path, device, transform)\n",
    "print(f\"Predicted Register Number: {predicted_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f456cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
