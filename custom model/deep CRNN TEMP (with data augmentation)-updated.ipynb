{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75247114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4276, Val: 535, Test: 535\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set up the data directory\n",
    "data_dir = 'data/Register Numbers/'\n",
    "\n",
    "# Load image paths and labels\n",
    "image_pairs = [\n",
    "    (os.path.join(data_dir, f), f.split('.')[0]) \n",
    "    for f in os.listdir(data_dir) \n",
    "    if f.endswith('.png')\n",
    "]\n",
    "\n",
    "# Validate labels\n",
    "def validate_label(label):\n",
    "    if len(label) != 12 or not label.isdigit():\n",
    "        raise ValueError(f\"Invalid register number: {label}\")\n",
    "    return label\n",
    "\n",
    "cleaned_pairs = [(path, validate_label(label[:12])) for path, label in image_pairs]\n",
    "\n",
    "# Split the data\n",
    "random.shuffle(cleaned_pairs)\n",
    "total = len(cleaned_pairs)\n",
    "train = cleaned_pairs[:int(0.8 * total)]\n",
    "val = cleaned_pairs[int(0.8 * total):int(0.9 * total)]\n",
    "test = cleaned_pairs[int(0.9 * total):]\n",
    "print(f\"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ef5caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegisterNumberDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('L')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label_seq = [int(digit) + 1 for digit in label]  # 0->1, 1->2, ..., 9->10\n",
    "        return image, torch.tensor(label_seq, dtype=torch.long)\n",
    "\n",
    "# Custom transform for random brightness\n",
    "class RandomBrightness(object):\n",
    "    def __init__(self, delta=0.3):\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, img):\n",
    "        brightness = random.uniform(-self.delta, self.delta)\n",
    "        img = torch.tensor(np.array(img)).float() / 255.0  # Convert to tensor [0,1]\n",
    "        img = img + brightness\n",
    "        img = torch.clamp(img, 0, 1)  # Keep values between 0 and 1\n",
    "        return Image.fromarray((img.numpy() * 255).astype(np.uint8))\n",
    "\n",
    "# Training transform with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(15),  # Rotate between -15 and 15 degrees\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.8, 1.2)),  # Shift and scale\n",
    "    RandomBrightness(0.3),  # Random brightness adjustment\n",
    "    transforms.GaussianBlur(kernel_size=3),  # Slight blur\n",
    "    transforms.Resize((32, 256)),  # Ensure consistent size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    transforms.Lambda(lambda x: x + 0.05 * torch.randn_like(x)),  # Add noise\n",
    "    transforms.Lambda(lambda x: torch.clamp(x, -1, 1)),  # Clamp after noise\n",
    "])\n",
    "\n",
    "# Validation/test transform (no augmentation)\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = RegisterNumberDataset([p for p, _ in train], [l for _, l in train], transform=train_transform)\n",
    "val_dataset = RegisterNumberDataset([p for p, _ in val], [l for _, l in val], transform=val_test_transform)\n",
    "test_dataset = RegisterNumberDataset([p for p, _ in test], [l for _, l in test], transform=val_test_transform)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8629e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Dropout2d(p=0.3, inplace=False)\n",
       "    (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): ReLU()\n",
       "    (22): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (23): Dropout2d(p=0.3, inplace=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(2, 1), stride=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU()\n",
       "  )\n",
       "  (rnn): LSTM(512, 256, num_layers=2, dropout=0.3, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CRNN, self).__init__()\n",
    "        # CNN component with dropout\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # (N, 64, 16, W/2)\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # (N, 128, 8, W/4)\n",
    "            nn.Dropout2d(0.3),  # Dropout after second maxpool\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,1), (2,1)),  # (N, 256, 4, W/4)\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,1), (2,1)),  # (N, 512, 2, W/4)\n",
    "            nn.Dropout2d(0.3),  # Dropout after fourth maxpool\n",
    "            nn.Conv2d(512, 512, kernel_size=(2,1)),  # (N, 512, 1, W/4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # LSTM with dropout between layers\n",
    "        self.rnn = nn.LSTM(512, 256, num_layers=2, bidirectional=True, dropout=0.3)\n",
    "        # Dropout before the fully connected layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512, num_classes)  # 512 because bidirectional (256 * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through CNN\n",
    "        x = self.cnn(x)  # (N, 512, 1, W/4)\n",
    "        x = x.squeeze(2)  # (N, 512, W/4)\n",
    "        x = x.permute(2, 0, 1)  # (W/4, N, 512) for LSTM\n",
    "        # Pass through LSTM\n",
    "        x, _ = self.rnn(x)  # (W/4, N, 512)\n",
    "        # Apply dropout before FC\n",
    "        x = self.dropout(x)\n",
    "        # Fully connected layer for classification\n",
    "        x = self.fc(x)  # (W/4, N, num_classes)\n",
    "        return x\n",
    "\n",
    "# Initialize the model (assuming 11 classes: blank + digits 0-9)\n",
    "model = CRNN(num_classes=11)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b9ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80, Train Loss: 1.5069, Val Loss: 1.1438, Train Acc: 0.00%, Val Acc: 0.00%\n",
      "Epoch 2/80, Train Loss: 1.0619, Val Loss: 1.0175, Train Acc: 0.00%, Val Acc: 0.00%\n",
      "Epoch 3/80, Train Loss: 1.0156, Val Loss: 1.0205, Train Acc: 0.00%, Val Acc: 0.00%\n",
      "Epoch 4/80, Train Loss: 0.9952, Val Loss: 0.9392, Train Acc: 0.00%, Val Acc: 0.00%\n",
      "Epoch 5/80, Train Loss: 0.9346, Val Loss: 0.8521, Train Acc: 0.00%, Val Acc: 0.00%\n",
      "Epoch 6/80, Train Loss: 0.8544, Val Loss: 0.7848, Train Acc: 0.00%, Val Acc: 0.00%\n",
      "New best model saved at epoch 7 with Val Acc: 0.37%\n",
      "Epoch 7/80, Train Loss: 0.7880, Val Loss: 0.6812, Train Acc: 0.05%, Val Acc: 0.37%\n",
      "Epoch 8/80, Train Loss: 0.7105, Val Loss: 0.6210, Train Acc: 0.23%, Val Acc: 0.00%\n",
      "New best model saved at epoch 9 with Val Acc: 1.68%\n",
      "Epoch 9/80, Train Loss: 0.6395, Val Loss: 0.5423, Train Acc: 0.33%, Val Acc: 1.68%\n",
      "New best model saved at epoch 10 with Val Acc: 2.24%\n",
      "Epoch 10/80, Train Loss: 0.5561, Val Loss: 0.4393, Train Acc: 1.43%, Val Acc: 2.24%\n",
      "New best model saved at epoch 11 with Val Acc: 11.40%\n",
      "Epoch 11/80, Train Loss: 0.4808, Val Loss: 0.3582, Train Acc: 4.21%, Val Acc: 11.40%\n",
      "New best model saved at epoch 12 with Val Acc: 17.38%\n",
      "Epoch 12/80, Train Loss: 0.4289, Val Loss: 0.3288, Train Acc: 7.55%, Val Acc: 17.38%\n",
      "New best model saved at epoch 13 with Val Acc: 26.36%\n",
      "Epoch 13/80, Train Loss: 0.3896, Val Loss: 0.2606, Train Acc: 12.28%, Val Acc: 26.36%\n",
      "New best model saved at epoch 14 with Val Acc: 44.49%\n",
      "Epoch 14/80, Train Loss: 0.3487, Val Loss: 0.1967, Train Acc: 17.94%, Val Acc: 44.49%\n",
      "New best model saved at epoch 15 with Val Acc: 46.54%\n",
      "Epoch 15/80, Train Loss: 0.3247, Val Loss: 0.1865, Train Acc: 20.72%, Val Acc: 46.54%\n",
      "New best model saved at epoch 16 with Val Acc: 62.62%\n",
      "Epoch 16/80, Train Loss: 0.3040, Val Loss: 0.1390, Train Acc: 25.96%, Val Acc: 62.62%\n",
      "New best model saved at epoch 17 with Val Acc: 65.23%\n",
      "Epoch 17/80, Train Loss: 0.2821, Val Loss: 0.1223, Train Acc: 29.30%, Val Acc: 65.23%\n",
      "New best model saved at epoch 18 with Val Acc: 69.53%\n",
      "Epoch 18/80, Train Loss: 0.2680, Val Loss: 0.1156, Train Acc: 31.81%, Val Acc: 69.53%\n",
      "Epoch 19/80, Train Loss: 0.2444, Val Loss: 0.1121, Train Acc: 36.09%, Val Acc: 67.85%\n",
      "Epoch 20/80, Train Loss: 0.2483, Val Loss: 0.1178, Train Acc: 36.06%, Val Acc: 69.53%\n",
      "New best model saved at epoch 21 with Val Acc: 75.70%\n",
      "Epoch 21/80, Train Loss: 0.2291, Val Loss: 0.0910, Train Acc: 38.77%, Val Acc: 75.70%\n",
      "New best model saved at epoch 22 with Val Acc: 76.26%\n",
      "Epoch 22/80, Train Loss: 0.2212, Val Loss: 0.0819, Train Acc: 40.43%, Val Acc: 76.26%\n",
      "New best model saved at epoch 23 with Val Acc: 80.37%\n",
      "Epoch 23/80, Train Loss: 0.2124, Val Loss: 0.0738, Train Acc: 41.30%, Val Acc: 80.37%\n",
      "Epoch 24/80, Train Loss: 0.2065, Val Loss: 0.0795, Train Acc: 43.19%, Val Acc: 80.00%\n",
      "Epoch 25/80, Train Loss: 0.2012, Val Loss: 0.0768, Train Acc: 43.45%, Val Acc: 77.57%\n",
      "Epoch 26/80, Train Loss: 0.1971, Val Loss: 0.0795, Train Acc: 45.09%, Val Acc: 78.69%\n",
      "New best model saved at epoch 27 with Val Acc: 82.80%\n",
      "Epoch 27/80, Train Loss: 0.1911, Val Loss: 0.0629, Train Acc: 46.49%, Val Acc: 82.80%\n",
      "Epoch 28/80, Train Loss: 0.1833, Val Loss: 0.0651, Train Acc: 47.90%, Val Acc: 79.25%\n",
      "New best model saved at epoch 29 with Val Acc: 83.74%\n",
      "Epoch 29/80, Train Loss: 0.1843, Val Loss: 0.0618, Train Acc: 47.54%, Val Acc: 83.74%\n",
      "New best model saved at epoch 30 with Val Acc: 85.61%\n",
      "Epoch 30/80, Train Loss: 0.1846, Val Loss: 0.0551, Train Acc: 48.11%, Val Acc: 85.61%\n",
      "New best model saved at epoch 31 with Val Acc: 86.73%\n",
      "Epoch 31/80, Train Loss: 0.1781, Val Loss: 0.0562, Train Acc: 51.17%, Val Acc: 86.73%\n",
      "Epoch 32/80, Train Loss: 0.1776, Val Loss: 0.0560, Train Acc: 49.65%, Val Acc: 85.42%\n",
      "New best model saved at epoch 33 with Val Acc: 87.66%\n",
      "Epoch 33/80, Train Loss: 0.1677, Val Loss: 0.0509, Train Acc: 53.09%, Val Acc: 87.66%\n",
      "Epoch 34/80, Train Loss: 0.1624, Val Loss: 0.0526, Train Acc: 52.97%, Val Acc: 86.36%\n",
      "New best model saved at epoch 35 with Val Acc: 90.09%\n",
      "Epoch 35/80, Train Loss: 0.1648, Val Loss: 0.0452, Train Acc: 53.91%, Val Acc: 90.09%\n",
      "Epoch 36/80, Train Loss: 0.1610, Val Loss: 0.0493, Train Acc: 55.00%, Val Acc: 88.97%\n",
      "New best model saved at epoch 37 with Val Acc: 92.34%\n",
      "Epoch 37/80, Train Loss: 0.1513, Val Loss: 0.0409, Train Acc: 57.81%, Val Acc: 92.34%\n",
      "Epoch 38/80, Train Loss: 0.1553, Val Loss: 0.0495, Train Acc: 56.55%, Val Acc: 89.72%\n",
      "Epoch 39/80, Train Loss: 0.1499, Val Loss: 0.0414, Train Acc: 57.72%, Val Acc: 91.40%\n",
      "Epoch 40/80, Train Loss: 0.1470, Val Loss: 0.0485, Train Acc: 58.00%, Val Acc: 89.72%\n",
      "Epoch 41/80, Train Loss: 0.1472, Val Loss: 0.0410, Train Acc: 57.58%, Val Acc: 91.59%\n",
      "New best model saved at epoch 42 with Val Acc: 92.71%\n",
      "Epoch 42/80, Train Loss: 0.1474, Val Loss: 0.0360, Train Acc: 57.11%, Val Acc: 92.71%\n",
      "Epoch 43/80, Train Loss: 0.1510, Val Loss: 0.0370, Train Acc: 57.53%, Val Acc: 92.71%\n",
      "Epoch 44/80, Train Loss: 0.1427, Val Loss: 0.0409, Train Acc: 58.51%, Val Acc: 91.96%\n",
      "New best model saved at epoch 45 with Val Acc: 92.90%\n",
      "Epoch 45/80, Train Loss: 0.1334, Val Loss: 0.0367, Train Acc: 59.94%, Val Acc: 92.90%\n",
      "Epoch 46/80, Train Loss: 0.1380, Val Loss: 0.0360, Train Acc: 60.45%, Val Acc: 92.90%\n",
      "New best model saved at epoch 47 with Val Acc: 94.02%\n",
      "Epoch 47/80, Train Loss: 0.1319, Val Loss: 0.0330, Train Acc: 60.36%, Val Acc: 94.02%\n",
      "Epoch 48/80, Train Loss: 0.1329, Val Loss: 0.0361, Train Acc: 61.81%, Val Acc: 92.71%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CTCLoss(blank=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Variables for tracking best model\n",
    "best_val_accuracy = 0.0\n",
    "best_model_path = 'crnn_register_best.pth'\n",
    "\n",
    "# Decoding function (used in both training and evaluation)\n",
    "def decode_predictions(outputs):\n",
    "    outputs = outputs.softmax(2).argmax(2)  # (T, N)\n",
    "    batch_size = outputs.size(1)\n",
    "    decoded = []\n",
    "    for b in range(batch_size):\n",
    "        seq = outputs[:, b].cpu().numpy()\n",
    "        prev = -1\n",
    "        result = []\n",
    "        for s in seq:\n",
    "            if s != 0 and s != prev:\n",
    "                result.append(s - 1)\n",
    "            prev = s\n",
    "        decoded.append(''.join(map(str, result)))\n",
    "    return decoded\n",
    "\n",
    "# Training loop with validation\n",
    "num_epochs = 80\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # (W/4, N, num_classes)\n",
    "        outputs_log = outputs.log_softmax(2)\n",
    "        batch_size = images.size(0)\n",
    "        input_lengths = torch.full((batch_size,), outputs.size(0), dtype=torch.long, device=device)\n",
    "        target_lengths = torch.full((batch_size,), 12, dtype=torch.long, device=device)\n",
    "        targets_flat = targets.view(-1)\n",
    "        loss = criterion(outputs_log, targets_flat, input_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Compute training accuracy\n",
    "        predictions = decode_predictions(outputs)\n",
    "        target_labels = [''.join(str(d - 1) for d in t.tolist()) for t in targets]\n",
    "        for pred, target in zip(predictions, target_labels):\n",
    "            if pred == target:\n",
    "                train_correct += 1\n",
    "            train_total += 1\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = train_correct / train_total * 100\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs_log = outputs.log_softmax(2)\n",
    "            batch_size = images.size(0)\n",
    "            input_lengths = torch.full((batch_size,), outputs.size(0), dtype=torch.long, device=device)\n",
    "            target_lengths = torch.full((batch_size,), 12, dtype=torch.long, device=device)\n",
    "            targets_flat = targets.view(-1)\n",
    "            loss = criterion(outputs_log, targets_flat, input_lengths, target_lengths)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            predictions = decode_predictions(outputs)\n",
    "            target_labels = [''.join(str(d - 1) for d in t.tolist()) for t in targets]\n",
    "            for pred, target in zip(predictions, target_labels):\n",
    "                if pred == target:\n",
    "                    val_correct += 1\n",
    "                val_total += 1\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = val_correct / val_total * 100\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Save model if validation accuracy improves\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': train_loss\n",
    "        }, best_model_path)\n",
    "        print(f\"New best model saved at epoch {epoch+1} with Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58268529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.90%\n"
     ]
    }
   ],
   "source": [
    "def decode_predictions(outputs):\n",
    "    outputs = outputs.softmax(2).argmax(2)  # (T, N) -> e.g., (64, N)\n",
    "    batch_size = outputs.size(1)\n",
    "    decoded = []\n",
    "    for b in range(batch_size):\n",
    "        seq = outputs[:, b].cpu().numpy()  # (T,)\n",
    "        prev = -1\n",
    "        result = []\n",
    "        for s in seq:\n",
    "            if s != 0 and s != prev:\n",
    "                result.append(s - 1)\n",
    "            prev = s\n",
    "        decoded.append(''.join(map(str, result)))\n",
    "    return decoded\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)  # (T, N, num_classes)\n",
    "        predictions = decode_predictions(outputs)\n",
    "        target_labels = [''.join(str(d - 1) for d in t.tolist()) for t in targets]\n",
    "        for pred, target in zip(predictions, target_labels):\n",
    "            if pred == target:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(f\"Test Accuracy: {correct/total*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Accuracy: 92.15% for 70 epochs\n",
    "\n",
    "# Test Accuracy: 92.90% for 75 epochs\n",
    "\n",
    "#Test Accuracy: 88.41% for 120 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c4da4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Register Number: 212223120098\n"
     ]
    }
   ],
   "source": [
    "def predict_register_number(model, image_path, device, transform):\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # (1, 1, 32, 256)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        output = model(image)  # (T, 1, num_classes), e.g., (64, 1, 11)\n",
    "        output = output.squeeze(1)  # (T, num_classes)\n",
    "        output = output.softmax(1).argmax(1)  # (T,)\n",
    "        seq = output.cpu().numpy()\n",
    "        prev = -1\n",
    "        result = []\n",
    "        for s in seq:\n",
    "            if s != 0 and s != prev:\n",
    "                result.append(s - 1)\n",
    "            prev = s\n",
    "    return ''.join(map(str, result))\n",
    "\n",
    "# Test with your image\n",
    "image_path = 'my_test.jpeg'  # Replace with your image path\n",
    "predicted_number = predict_register_number(model, image_path, device, val_test_transform)\n",
    "print(f\"Predicted Register Number: {predicted_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87573f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Register Number: 212223240068 for 92.15% accurate model\n",
    "# for my_reg.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d68138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
